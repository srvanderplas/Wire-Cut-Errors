---
title: "Multiple Comparisons in Toolmark Evidence"
author: "Susan Vanderplas, Heike Hofmann"
format: pdf
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```

## Introduction

-   Problems with database searches and multiple comparisons in forensics
-   Fingerprint - madrid bombing
-   DNA databases
-   Firearms and toolmarks?? NIBIN/IBIS issues, but more broadly there are issues

One of the common suggestions in the 2016 PCAST report across pattern disciplines in forensic science was the importance of creating huge databases of e.g. known prints and firearms to support research into matches and closest non-matches[@pcast, fingerprints pg 11, 88, firearms pg 12]. As researchers, we concur with this suggestion, as it is critical for developing objective algorithms and creating useful black-box studies. We have spent considerable time assembling large data sets of tool marks, bullets, and cartridge cases to support our research as well as create public resources to accelerate future research [@csafeDataPortal]. However, we are wary about the implications of database searches across forensic disciplines, for reasons also mentioned in the PCAST report: the necessity of understanding the random match probability for searches against these databases, in particular when using degraded evidence often found in crime scenes [@pcast, pg 52].

This issue has been raised in the past with regard to DNA [@thompson2003] and fingerprints [@fine2006]; one of the many issues identified in the aftermath of the 2004 Madrid bombing case was that the IAFIS database used to locate similar prints is extremely large and thus it is possible for the database to locate 'unusually' similar non-matches. This issue will almost certainly become an issue in other pattern disciplines as databases of firearms evidence such as NIBIN @nibin2021 become more commonly utilized by law enforcement. In this paper, however, we examine a more subtle issue present in tool mark examinations: searches within evidence collected as part of a single case. We begin with a hypothetical set of three scenarios involving the collection of tool mark evidence and then assess the statistical issues involved with each scenario and explore possible resolutions to those issues.

### Examination Process

When an examiner conducts a tool mark assessment of wire cut evidence, they are typically working with a tool recovered from the suspect as well as a wire recovered from the scene. The examiner will typically take the tool and use it to cut a piece of metal, known as a blade cut, to record the markings of the entire cutting surface simultaneously. This process may be repeated at multiple angles in order to record the effect of different contact surfaces on the resulting striations. Markings on the wire will then be compared to these blade cuts using either visual inspection or perhaps in the future a numerical comparison algorithm similar to those developed for bullet striations [@hare2017automatic; @mattijssen2021] and cartridge cases [@zemmels2023; @tong2014; @geradts2001].

The examiner will compare blade cuts from all cutting surfaces during this process. Tools used for wire cuts typically have either 2 or 4 cutting surfaces. Examiners may make multiple cuts at multiple angles; this will increase the number of available blade cuts for comparison.

<!-- For simplicity, let us for now assume that only a single cut under one controlled angle was made. -->

<!-- ### Blade Configurations -->

<!-- There are 3 major types of blade configurations we observed -->

### Counting Comparisons

Let us consider a single blade cut from a single cutting surface. Define the blade cut to be of length $\ell$ and the wire to have diameter $d$; we will also assume that the wire is fully covered with striations suitable for comparison.

At minimum, there must be $\ell/d$ comparisons in order to assess whether the tool matches the wire; in this case, the comparisons are essentially independent of each other (conditional on the striation patterns) because no overlapping data is used. Of course, in this situation, we are overlooking the potential for partial matches and assuming that everything is lined up perfectly to produce this minimum number of comparisons; in practice, of course, the number of comparisons in this situation would likely be much higher.

At a maximum, both the blade and the wire are either digitally scanned at the same resolution of $r$ mm, or visually examined using a microscope with a digital view of a certain resolution which can be translated into an equivalent resolution to the digital scan. In this case, there may be as many as $\ell/r- d/r + 1$ comparisons performed in order to find the optimal alignment between the two samples[^1]. Of course, these comparisons are highly dependent, as each sequential comparison shares much of the same data with the previous and next comparisons. While it is likely possible to model this autocorrelation and compute an effective number of comparisons accounting for the autocorrelation, developing this model is outside the scope of this paper.

[^1]: This assumes we restrict the wire to be completely contained within the bounds of the blade surface. If we allow for partial matches and restrict these matches to require at least $k$ points of overlap, then this becomes $\ell/r - d/r + k + 1$.

Thus, we will consider the two extremes: $\ell/d$ completely independent comparisons and $\ell/r - d/r + 1$ dependent comparisons. The true number of visual or numerical comparisons lies somewhere between these two values.

## Thought Experiment

```{r shop-data}
library(tibble)
library(dplyr)
library(tidyr)
library(purrr)

cutting_surfaces <- tribble(
  ~type, ~length, ~surfaces, ~tools,
  "Pliers", 1.5, 2, 10,
  "Pliers", 1, 4, 3,
  "Side cutters", 1, 4, 1,
  "Side cutters", 2, 4, 2,
  "Scissors", 8, 2, 14,
  "Tin snips", 4, 2, 4,
  "Lineman pliers", 2, 4, 2,
  "Lineman pliers", 1, 4, 1,
  "Wire strippers", 1, 2, 2,
  "Long handled cutting pliers", 2, 4, 1,
  "Long handled cutting pliers", 3, 4, 1,
  "Utility knife blades", 6.2, 2, 50
) %>%
  mutate(total_cutting_length = length*surfaces*tools)

total_cutting_surface = sum(cutting_surfaces$total_cutting_length)

dad_surfaces <- tribble(
  ~type, ~length, ~surfaces, ~tools,
  "Pliers", 1.8, 2, 1,
  "Pliers", 1.6, 2, 1,
  "Pliers", 1.1, 2, 1,
  "Wire cutters", 2, 2, 1,
  "Wire cutters", 1.5, 2, 1,
  "Wire cutters", 1, 2, 2,
  "Other cutters", 8, 2, 1,
  "Other cutters", 4, 4, 1,
  "Scissors", 7.5, 2, 5,
  "Utility knife blades", 6.2, 2, 160,
  "Razor blades", 4, 2, 14,
  "Knife", 4, 2, 1,
  "Knife", 2, 2, 1,
  "Clippers", 4, 2, 1
) %>%
  mutate(total_cutting_length = length*surfaces*tools)

dad_total_cutting_surface = sum(dad_surfaces$total_cutting_length)
```

Consider, for example a situation where someone builds a bomb (badly), and a wire fragment is recovered from the un-detonated explosive device. We will examine how the forensic examination might proceed in three different cases:

1.  Examiners identify a suspect and locate a single tool which is believed to have been used to construct the bomb. Examiners would like to determine whether this tool is a forensic match to wires within the undetonated bomb.

2.  Examiners identify a suspect and locate a garage full of tools, one of which may have been used to construct the bomb. Examiners would like to identify which tool(s) were used to cut the wire during bomb construction.

3.  Examiners identify several suspect, each of whom has a set of tools, and wish to determine which suspect(s) may have constructed the bomb based off of the toolmark comparisons.

<!-- https://www.calmont.com/wp-content/uploads/calmont-eng-wire-gauge.pdf -->

<!-- https://en.wikipedia.org/wiki/American_wire_gauge -->

### Single Tool, Single Cut Comparisons

To make the calculations of number of comparisons more concrete, consider the pair of pliers shown in @fig-wire-cutter, which has a 1.5 cm cutting surface machined on both sides to produce a peaked cross-section. When used to cut a wire, the wire rests against a rectangular surface used to hold it in place; the blade is pushed into the wire, producing striae resulting from imperfections in side A on one half of the wire and striae from imperfections and wear in side B on the other.

![Wire cutter with 1.5 cm cutting surface beveled on both sides](../fig/wire_cutter.png){#fig-wire-cutter}

```{r}
resolution <- 0.645 # res in mum
length_blade <- 15 # length in mm
wire_diam <- 2 # diameter in mm
mm_to_mum <- 1000 # conversion

obs_blade <- round(length_blade * mm_to_mum/resolution)
obs_wire <- round(wire_diam * mm_to_mum/resolution)
alg_compare <- obs_blade - obs_wire + 1

examiner_increment <- wire_diam # increment in mm
examiner_compare <- (length_blade - wire_diam)/examiner_increment + 1
```

A scan of a cut made by the blade at $`r resolution`\mu m$ resolution will produce a sequence of approximately $`r length_blade` \times `r mm_to_mum`/`r resolution` = `r format(obs_blade, big.mark = ',')`$ observations. A scan of a 2 mm wire would produce $`r wire_diam` \times `r mm_to_mum`/`r resolution` = `r format(obs_wire, big.mark = ',')`$ observations.

In the minimal comparisons case, we would have `r examiner_compare` comparisons for each side of the blade's cutting surface for a total of `r 2*examiner_compare` comparisons.

In the extreme case, computationally aligning the wire sequence to one blade cut using maximum cross-correlation would require $`r obs_blade` - `r obs_wire` + 1 = `r format(alg_compare, big.mark = ',')`$ (obviously dependent) comparisons. As there are two or more blade cuts for each cutting tool, there are at most $2(`r obs_blade` - `r obs_wire` + 1) = `r format(2*alg_compare, big.mark = ',')`$ comparisons including the full wire surface.

While $`r examiner_compare*2`$ comparisons might not seem so terrible, this is the simplest of the 3 scenarios we consider; each scenario presents a progressively more complex problem.

### A Garage of Comparisons

In our next scenario, investigators identify a suspect and collect all of the tools in the suspect's residence which might have been used to construct the bomb.

Let us consider the garage of one of the authors of this paper; @fig-shop-layout shows the garage work bench and tool storage in typical condition. While there is no reason to think the author's garage is representative of all garages, the general setup and number of tools in the garage is not unusual for a suburban household where residents may dabble in automotive repair, home improvement, or woodworking.

::: {#fig-shop-layout layout-ncol="2"}
![Shop Bench Tool rack](../fig/PXL_20231110_190517827.jpg)

![Shop Side Tool rack](../fig/PXL_20231110_190522765.jpg)

Layout of tool storage in the author's garage. For this study, we considered only tools which might reasonably be used to cut wires.
:::

```{r}
#| label: tbl-shop-surfaces
#| tbl-cap: "Shop cutting surfaces. All lengths in cm."
#| echo: false
#| message: false
knitr::kable(cutting_surfaces %>% rename(Type = type, `Length (cm)` = length, `Cutting Surfaces` = surfaces, `# Tools` = tools, `Total Cutting Length (cm)` = total_cutting_length))
```

```{r}
#| label: tbl-shop-comparisons
#| tbl-cap: "Comparions performed by tool type, assuming digital scans at a resolution of $0.625\\mu m$ and visual comparisons performed at $0.2 mm$ intervals along the test blade impression."
#| echo: false
#| message: false
comparisons <- cutting_surfaces %>%
  mutate(digital = (length*10*mm_to_mum/resolution - wire_diam*mm_to_mum/resolution + 1)*surfaces*tools,
         analog = (length*10/examiner_increment - wire_diam/examiner_increment + 1)) %>%
  group_by(type) %>%
  summarize(digital = sum(round(digital)), analog = sum(round(analog))) %>%
  rename(`Tool type` = type, Algorithm = digital, Examiner = analog)
  
compare_tbl <- bind_rows(comparisons, tibble(`Tool type` = "Total", Algorithm = sum(comparisons$Algorithm), Examiner = sum(comparisons$Examiner)))
knitr::kable(compare_tbl, format.args = list(big.mark=','))
```

We assembled the easily available tools from the house and garage which might reasonably be used to cut wires[^2], including pliers, wire cutters and strippers, scissors, tongs, and utility knives. Estimated cutting surface length and quantity of each type of tool is provided in @tbl-shop-surfaces. The total length of all cutting surfaces of relevant tools in the author's garage is `r total_cutting_surface` cm; primary contributions to the total length are the 50 pack of utility knife blades and the 14 pairs of full-size scissors which could be located throughout the house. Of course, a thorough search of the garage and house, as one would expect from professionals investigating an actual crime, would yield far more cutting surfaces to compare against.

[^2]: Reasonably is defined as we (the authors) have used a tool like this to cut wires while doing home improvement projects in the past.

As in Scenario 1, the wire fragment recovered from the un-detonated explosive would be compared to blade surface test impressions generated from each cutting surface in the garage. However, unlike in Scenario 1, we have `r format(total_cutting_surface, big.mark=',')` cm of cutting surfaces to work with. We can apply our comparison formula on a by-surface basis to yield the digital and analog comparison estimates shown in @tbl-shop-comparisons. Overall, our algorithm would make `r format(sum(comparisons$Algorithm), big.mark=',')` (dependent) comparisons and an examiner doing a visual inspection would make `r format(sum(comparisons$Examiner), big.mark=',')` comparisons under the assumptions we made in Scenario 1.

### Multiple Suspects

To fully understand the magnitude of this problem, suppose that we have a group of 4 additional individuals who may have contributed to building the un-detonated bomb. These individuals are each associated with the author and have hobbies that include building electronic devices, home improvement, welding, and woodworking (thus, they each have sets of tools). As part of the investigation, each individuals' tools are confiscated and examined; blade cuts are made of each cutting surface to be matched to the wire in evidence. @tbl-garage-comparisons shows the number of comparisons which would need to be performed to ensure that all wire-cutting tools in each suspect's garage are compared to the evidence from the scene.

```{r}
#| label: tbl-garage-comparisons
#| tbl-cap: "Total cutting length of easily accessible wire-cutting tools for 4 individuals associated with the author."
#| echo: false
#| message: false
set.seed(42309427)


n_tools <- c(40, 20, 30)
n_blades <- sample(5:75, size = 3)

tool_options <- select(bind_rows(cutting_surfaces, dad_surfaces), 1:3) %>% unique()

blades <- filter(tool_options, type %in% c("Utility knife blades", "Razor blades"))

other_tools <- filter(tool_options, !type %in% c("Utility knife blades", "Razor blades"))

dad_tools <- dad_surfaces %>% 
  mutate(suspect = 4) %>% select(-total_cutting_length)

our_tools <- cutting_surfaces %>% mutate(suspect = 0) %>% select(-total_cutting_length)

sim_garages <- tibble(suspect = 1:3, n_tools = n_tools, n_blades = n_blades) %>%
  mutate(data = map2(
    n_tools, n_blades, 
    ~bind_rows(
      slice_sample(other_tools, n = .x, replace = T), 
      slice_sample(blades, n = .y, replace = T)) 
  )) %>%
  unnest(data) %>%
  group_by(suspect, type, length, surfaces) %>% 
  summarize(tools = n()) %>%
  ungroup()

garages <- bind_rows(sim_garages, dad_tools, our_tools) %>%
  group_by(suspect) %>%
   mutate(digital = (length*10*mm_to_mum/resolution - wire_diam*mm_to_mum/resolution + 1)*surfaces*tools,
         analog = (length*10/examiner_increment - wire_diam/examiner_increment + 1)*surfaces*tools) 

garage_sum <- garages %>%
  summarize(
    total_cutting_length = sum(length * surfaces * tools), 
    digital = sum(digital), 
    analog = sum(analog)) %>%
  mutate(suspect = as.character(suspect))

garage_sum_tot <- bind_rows(garage_sum, tibble(suspect = "Total", total_cutting_length = sum(garage_sum$total_cutting_length), digital = sum(garage_sum$digital), analog = sum(garage_sum$analog)))

garage_sum_tot %>%
  rename(Individual = suspect, `Total Cutting Length (cm)` = total_cutting_length, "Algorithm" = digital, "Examiner" = analog) %>%
  knitr::kable(format.args = list(big.mark = ','))
```

If investigators were to attempt to determine which of the suspected individuals cut the wire in evidence, they would need to do `r format(sum(garage_sum$analog), big.mark = ',')` distinct visual comparisons. Even if there is an incredibly small false positive error rate, the odds of a false identification are much more likely in this scenario than either of the previous two scenarios. But how likely? In the next section, we describe two different scenarios for calculating the overall likelihood of a false identification using simple probability calculations and estimated coincidental match probabilities as well as a theoretical approach designed to mimic the approach used with database searches that return the $N$ most similar results.

## The Problems with Database Searches

Let us first consider the simplest situation where a single tool is recovered from the suspect's house that is viewed as likely to have made the wire cut. In order to compare tool marks in this scenario, the examiner or algorithm completes the equivalent to a database search resulting from comparing a single wire in evidence to a single tool. This multiple comparison problem becomes more severe at each step of our thought experiment - considering a single shop of tools adds multiple orders of magnitude to the number of comparisons.

In addition, we might expect that if the guilty party in our thought experiment had succeeded in exploding the bomb, the evidence might be reduced in quality - the proportion of the wire with usable striae might be significantly smaller, which would compound the problem even further.

The dangers of database searches in forensics are well known; database searches have been implicated in forensic missteps such as the Madrid bombing case, as discussed in @li2023; @newman2007; @mayfield2006report [Pg 137]. In this section, we illustrate the problems with database searches in two different ways: first, considering the implications for random match probabilities under multiple comparisons, and second, considering the impact of "best $N$ matches" search queries on the distribution of match scores. These issues are separate, but related, and represent different ways searches and databases (formal or informal) might be used in practice. While law enforcement sanctioned wire or toolmark databases used for forensic intelligence do not currently exist to the best of our knowledge, similar systems exist for bullets and cartridge cases (IBIS), fingerprints (AFIS), and footwear (NFS, a UK database). It seems reasonable to consider both the informal databases created from an extensive investigation and the formal databases which might be created in the future.

### Probability of a False Identification

Let us briefly consider a non-exhaustive list of ways an examiner or algorithm might arrive at a false identification:

-   class characteristics match + decider doesn't recognize class characteristics as such

-   coincidental similarity between two items + decider doesn't see/recognize dissimilarities

-   the items randomly match on identifying characteristics

All of these situations may occur during black box studies (though the final situation is very improbable). As a result, perhaps the best estimate of the probability of a false identification that we can obtain is the false identification rate of one or more white box studies. We have previously published a paper detailing the error rates of different firearms and toolmark studies; to our knowledge there are no similar white box studies for wire cuts. However, if we confine ourselves to error rates for striated materials (bullets, aperture shear, ejectors, screwdrivers) we may be able to get a reasonable estimate for the error rate of wire cuts. We assembled 3 black-box error rate studies of striated toolmarks (bullets and firing pins) [@bajic2020; @mattijssen2021; @best2022]; these studies estimate the false positive rate (e.g. the number of different-source identifications relative to the total number of different-source comparisons) rate to be between 0.07235 [@mattijssen2021, firing pins only] and 0.004545 [@best2022]. While there are a number of problems with these studies (which we have detailed in previous papers, i.e. @inconclusives), our goal here is to use the best estimates we can get of false positive error rates in particular, as the false positive rate is the main driver of the problem with multiple comparisons. If we calculate a pooled false positive rate from all studies, we arrive at a false positive rate of $\frac{20 + 56 + 1}{2842 + 774 + 220} = `r sprintf('%0.02f', 77/(2842+774+220))`$, that is, about 2%.

```{r}
comparisons <- examiner_compare*2
fpr = (20+56+1)/(2842+774+220) 
fprformat = sprintf("%0.04f", fpr)
```

If we start out by considering the first scenario (a single tool with a 1.5 cm blade compared to a single 2mm wire) and completely independent non-overlapping comparisons (7.5 comparisons) with only 2 cutting surfaces, we would effectively have 15 independent comparisons. Using this information, we can calculate the probability of at least one false positive error to be `r sprintf("%0.04f%%", 1-(1-fpr)^comparisons * 100)` as shown in Equation \ref{eq:fpr}. While far more comparisons are likely to be performed, these comparisons may have some dependencies (at which point our formula would need to account for the covariance between successive comparisons). Ultimately, the number of comparisons cannot drop below `r comparisons` without leaving some evidence unexamined, which means this overall false positive rate is the lower bound.

This is the simplest situation of the three different scenarios we have considered, and we still have an unacceptably high rate of at least one false positive conclusion. If the wire size decreases or the blade length increases, the overall chance of at least one false positive error will increase because the number of comparisons increases. If examiners consider cuts made at multiple angles, the same conclusion holds - the chance of at least one false positive error will increase from this point.

```{=tex}
\begin{align*}
P(\text{False positive error in one comparison}) &= `r fprformat`\\
P(\text{No false positive error in one comparison}) &= `r sprintf('%0.04f', 1-fpr)`\\
P(\text{no false positive errors in } N \text{ comparisons}) &= `r sprintf('%0.04f', 1-fpr)`^N\\
P(\text{at least one false positive error in } N \text{ comparisons}) &= 1 - `r sprintf('%0.04f', 1-fpr)`^N  \tag{A}\label{eq:fpr}\\
&= `r sprintf("%.04f", 1-(1-fpr)^comparisons)` %>% 
\end{align*}
```
It then becomes relevant to consider how low the error rate would need to be for striated comparisons in order to make wire cut examination plausible. If we control the overall error rate amid $N$ independent comparisons, our estimated FPR for a single comparison can be calculated by solving Equation \ref{eq:fpr-control} for $r$.

Let $F_n:=\{\text{at least one false positive in }n\text{ comparisons}\}$

```{=tex}
\begin{align*}
P(F_1) &= r\\
P(\overline{F_1}) &= 1-r\\
P(F_n) &= \left[1-r\right]^n\\
P(\overline{F_n}) &= 1 - \left[P(\overline{F_1})\right]^n = 1 - \left[1 - r\right]^n\tag{B}\label{eq:fpr-control}\\
r &= 1 - \left[1 - P(F_n)\right]^{1/n}
\end{align*}
```
We can explore what this means visually by considering different values of $N$ and $r$ to see how the overall false positive error rate behaves, as shown in @fig-control-overall-fpr. The astonishing conclusion is that even at the smallest error rate for striated comparisons, 0.45% [@best2022], we have about a 3% chance of at least one false positive conclusion when 10 comparisons are made and about a 37.5% chance of at least one false positive conclusion when 100 comparisons are made! These results suggest that under current estimates of false positive error rates, it is nearly impossible for toolmark comparisons of wires to provide compelling, "beyond a reasonable doubt" levels of evidence.

```{r control-overall-fpr}
#| echo: false
#| message: false
#| fig-cap: Exploration of overall error rate estimates for different values of $r$, the false positive rate for a single comparison. Dotted vertical lines show false positive error rate estimates (from left to right, @bajic2020; @best2022; @mattijssen2021). 
#| label: fig-control-overall-fpr
library(ggplot2)
library(tidyr)
single_rmp = exp(seq(-1000, 0, .1))
N = c(10, 100, 1000, 10000)

res = crossing(single_rmp, N) %>%
  mutate(overall = 1 - (1 - single_rmp)^N)

ggplot(res, aes(x = single_rmp, y = overall, group = factor(N), color = factor(N))) + 
  geom_line() +
  coord_cartesian(xlim = c(0, 0.07), ylim = c(0, 1)) + 
  ylab("Probability of at least one false positive\nin N comparisons") + 
  xlab("Probability of false positive for 1 comparison") + 
  ggtitle("Wire Comparison: Controlling Overall False Positive Rate") + 
  geom_vline(xintercept = 0.02, linetype = "dashed", color = "grey40") + 
  geom_vline(xintercept = c(0.00704, 0.0724, 0.004545), linetype = "dotted", color = "grey50") + 
  annotate(geom = "text", x = 0.021, y = 0.125, label = "Striated toolmark error rate estimate\nused in this paper", color = "grey40", hjust=0) + 
  scale_color_discrete("# comparisons") + 
  theme_bw()

```

### Score-based Distributions

## Potential Solutions

We need to have an explicit relationship between false positive rate and amount of signal (assessed by length, number of features, overall striae depth, etc.). Until we have this kind of relationship, it will be hard to use error rates (even if they were known for toolmarks) from algorithms or black-box studies in practical situations.

We need much more study of real data, examiner evaluations, and performance of algorithms in situations which mimic real casework. We also need to quantify the strength, distinctiveness, and total number of striations to ensure that comparisons are being made only in situations where random matches are unlikely to occur.
