\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\usepackage{cleveref}
\usepackage[dvipsnames]{xcolor} % colors
\newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
\newcommand{\hh}[1]{{\textcolor{ForestGreen}{#1}}}
\newcommand{\ac}[1]{{\textcolor{Purple}{#1}}}
\templatetype{pnasbriefreport}
%\templatetype{pnasbriefreport} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Mirror, mirror on the wall. Which's the best match of them all?}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1,2]{Susan Vanderplas}
\author[b, c]{Alicia Carriquiry}
\author[b, c, 1]{Heike Hofmann}

\affil[a]{Statistics Department, University of Nebraska Lincoln. 350 Hardin Hall, 3310 Holdrege North Wing, Lincoln, NE 68503}
\affil[b]{Department of Statistics, Iowa State University. 1121 Snedecor Hall, 2438 Osborn Dr, Ames, IA 50011}
\affil[c]{Center for Statistics and Applications in Forensic Evidence. 195 Durham Center, 613 Morrill Road, Ames, Iowa 50011}

% Please give the surname of the lead author for the running footer
\leadauthor{Vanderplas}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{The authors have no competing interests to declare.}
\equalauthors{\textsuperscript{1}SVP (Author One) contributed equally to this work with HH (Author Two).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: susan.vanderplas@unl.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{Forensic Evidence $|$ Statistics $|$ Wire cuts $|$ Toolmark analysis}


\begin{abstract}

\ac{When wires are cut, the tool produces striations on the cut surfaced}; as in other forms of toolmark analysis, these striae are used to connect the wire to the tool that was used \ac{in the cutting process. Here, we argue that the practice} of comparing two wire cut surfaces introduces complexities not present in better-investigated forensic examination  of toolmars such as those observed on bullets, as wire comparisons inherently require multiple distinct comparisons, increasing the expected false positive rate. We call attention to the multiple comparison problem in wire examination and relate it to other situations in forensics that involve multiple comparisons, such as database searches.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}
<<setup, include = F>>=
library(knitr)
opts_chunk$set(error=F, warning=F, message = F, dpi=300, echo = F)
options(digits = 4)
@

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}


\firstpage[3]{6}
% Use \firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. "\firstpage[4]{11}".


% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
% \dropcap{T}his PNAS journal template is provided to help you write your work in the correct journal format. Instructions for use are provided below.

\dropcap{I}n forensic \ac{evaluations}, a single conclusion often \ac{relies} on  many comparisons, either implicitly or explicitly.
Multiple comparisons arise persistently when developing statistical methods to address scientific problems \citep{benjaminiControllingFalseDiscovery1995}, and can significantly increase the probability of false "discoveries".  Yet, this potentially significant issue is often ignored.  \ac{Now that vast databases and efficient algorithms are routinely used in forensic evaluations to find "matches" to crime scene items, the problems of close non-matches\citep{pcast} and the erosion of confidence due to multiple comparisons become critically important.}
%Researchers have been focusing on building up databases to develop algorithms for objective evidence comparisons.
%\svp{Then, the algorithms facilitate} searches across (vast) databases, reporting the best $N$ matches to a particular piece of evidence.
%This raises the critically important problem of .

%and explore closest non-matches . While these databases (and open data) are critically important in forensics, we also need to be careful about how our methods are affected by the multiple comparison problem.

A similar concern was raised in the past with regard to DNA \citep{thompson2003} and latent print evaluations  \citep{koehler2021}: one of the issues \citep{fine2006} leading to the wrongful accusation of Brandon Mayfield in the 2004 Madrid train bombing case was that the large size of the IAFIS database used to search for similar prints made it possible to locate `unusually' close non-matches.
\ac{The probability of finding  close non-matches for an item of evidence is directly proportional to the size of the database available for searching;} false "matches" occur more frequently as databases increase in size and algorithms become more powerful.
% In this paper, we discuss that very close non-matches are far from an unusual phenomenon  but a direct consequence of a large number of comparisons, and simply reflect the power of the search algorithms.

%
% \hh{Database searches are an example where the number of searches is explicitly available as the number of objects meeting the search criteria. } % This doesn't seem super-relevant and distracts from the continuity between the algorithm discussion above and the next paragraph

%\hh{XXX director's notes: XXX what is different between full and partial matches is that error rates in the literature are generally reported based on full length comparisons, where macro features are used to anchor comparisons (align along grooves, then zoom in and only 'wiggle' a bit to the left and right to adjust at the micro-scale). These kind of adjustments are 'baked' into published error rates. }

\svp{Compounding this issue,} the use of novel algorithms typically results in a large number of comparisons that may not obvious to the user.
For example, the cross-correlation function\citep{vorburgerApplicationsCrosscorrelationFunctions2011} was one of the first measures proposed for quantifying the similarity  between two patterns in response to the \ac{ 2009 NRC} report \citep{nas2009}, and continues to be used in many pattern searching algorithms to find the best alignment between two images and also to quantify their overall similarity.
Finding the best alignment often consists in sliding one surfaceacross the whole length (for one-dimensional patterns, such as striations) or area (for two dimensional sources, such as impression marks) of the other item while keeping track of the value of a similarity measure.
This mirrors the forensic examination process, where examiners visually rotate or align two surfaces under comparison microscopes.
In order to avoid \ac{problematic} false accusations and a further erosion of the public's trust in science, it is important to address the problem of multiple comparisons by controlling their effect on false discovery rates.

Here, we consider the multiple comparisons problem that arises from a relatively simple toolmark examination: matching a cut wire to a wire-cutting tool.
We describe the comparison approach, estimate the (minimal) number of comparisons that are needed to carry out the examination, and discuss how the false discovery rate changes with the number of comparisons involved, using error rates derived from published black-box studies.


\section*{Examination Process}

A forensics examiner tasked with determining whether a wire in evidence was cut by a specific tool will create one or more blade cuts \ac{using the putative tool,} which can be compared to the cut surface of the wire.
These cuts are made in a sheet of material similar to the wire's composition, and may be performed at multiple angles, as the angle of the tool to the substrate can affect which striations are recorded on the substrate surface.
The blade cuts will then be compared to the wire under a comparison microscope, though eventually, automatic comparison algorithms may also be validated for lab use.
Each side of each blade cut will be compared to each side of the wire; depending on the tool design, there can be between two and four cutting surfaces in contact with the substrate.

\section*{Calculating the Number of Comparisons}

In order to calculate the number of possible comparisons to be carried out in the course of the examination, we define $\ell$ to be the length of the blade cut, and $d$ to be the diameter of the wire. Without loss of generality, we assume that the wire is fully covered with striations suitable for comparison; if this is not the case, we adjust the value of $d$.
\svp{An illustration of the sliding comparison process is shown in \Cref{fig:wire-blade}.} Imagine that you start from the left end of the blade and move the wire surface \ac{$d$ units at a time} along the blade.
At minimum, there must be $\ell/d$ comparisons in order to assess whether one side of the blade cut matches one side of the wire; in this case, the comparisons are independent of each other (conditional on striation patterns) because \ac{each comparison involves different surfaces.}
%no overlapping data is used.
Of course, this overlooks the possibility of partial matches and assumes perfect alignment between the diameter of the wire and the portion of the blade used to cut the wire, which is unrealistic.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{../fig/wire-blade-comparison-label.png}
\caption{A comparison between a wire and a blade cut requires sliding the wire along the entire blade cut length to determine the best match (or whether there is a match). \svp{Surfaces shown are rendered 2D topographical scans of a wire and blade cut taken with a confocal light microscope.}}\label{fig:wire-blade}
\end{figure}

At a maximum, both the blade and the wire are either digitally scanned at resolution $r$ mm, or visually examined using a microscope with a digital resolution that can be expressed as $r$ equivalent to the digital scan.
In this case, there may be as many as $\ell/r - d/r + 1$ comparisons performed in order to find the optimal alignment between the two samples.
These comparisons are \ac{no longer independent}, as each sequential comparison shares much of the same data with the previous and next comparisons.
% ; such dependency could be expressed via autocorrelation, but we will defer that derivation to a later paper.
\svp{For simplicity}, let us consider the number of comparisons to lie somewhere between \svp{these two estimates}.

\ac{Finally}, we must consider the number of surfaces which must be compared: the wire may have between one and two sets of striae (depending on tool configuration) and there may be between 2 and 4 different blade cut surfaces to examine.
This results in a multiplier of between 2 and 8 comparisons to be conducted to compare the striations on the wire to the striations on the blade cuts.

\subsection*{A concrete example}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{../fig/wire_cutter_wires.png}
\caption{RJ45 Crimp tool with a 1.5cm razor blade used for cutting. 1mm and 2mm diameter aluminum wires cut with the pliers are shown in a box in the top right corner.}\label{fig:tool-pic}
\end{figure}

Let us consider a wire-cutting tool with a 1.5 cm razor blade that meets a cast surface (one such tool is shown in \Cref{fig:tool-pic}); the wire is held against this rectangular cast surface as the blade is pushed into the wire, splitting it in two.
This is a minimal scenario - the wire will acquire striations from one side of the blade, while the blade itself has two cutting edges, which we will call side A and side B.
The blade cuts of a sheet of aluminum that is 1.5 cm in width thus produce two striated edges corresponding to side A and side B which can be compared to other cuts to assess similarity.

In addition, we have a 12 gauge aluminum wire (2 mm diameter) which may have been cut  by the wire-cutting tool described above.
Class characteristics, which are shared by all tools of similar manufacture, appear to match:
there is a flat impression on one side of the wire corresponding to the cast metal backstop of the tool,
and the wire is cut such that the blade and the backstop appear to be perpendicular
(that is, the wire appears to have been cut with a blade similar to the blade configuration of the tool in question).

<<simple-problem-setup, echo = F, warning = F, message = F>>=
ell <- 15
d <- 2
res <- 0.645/1000
ncomp_min <- ell/d
ncomp_max <- ell/res - d/res + 1
@


In this example, $\ell = \Sexpr{ell} mm$, $d = \Sexpr{2} mm$, and the minimal number of comparisons between the wire and one blade cut is $\ell/d = \Sexpr{ncomp_min}$.
As there are two blade cuts (side A and side B), the minimal number of comparisons overall is \Sexpr{ncomp_min*2}, and these comparisons are \ac{non-overlapping and} independent.

If the blade cuts and the wire edge are scanned at a resolution of $\Sexpr{res*1000} \mu m$, the resulting digital files will require \svp{approximately $\Sexpr{format(round(ncomp_max/1000)*1000, big.mark=',')}$ comparisons per blade cut; $\Sexpr{format(round(ncomp_max/1000)*2000, big.mark=',')}$ comparisons overall}.
These comparisons are required in order to find the optimal alignment between the wire and the blade cut; they are implicit in the calculation of cross-correlation which is the first \ac{and often} the only step used to objectively assess the similarity between striated evidence such as bullets, aperture shear, and firing pin impressions.
This problem is not unique to algorithms; an examiner would need to physically align the wire and the blade cut by searching along the length of the cut to visually match striations, performing the same process physically that the algorithm performs computationally.
While these sequential comparisons are highly autocorrelated, and we cannot assume sequential independence when calculating the probability of an error, they serve as an upper bound on the number of comparisons which could be performed.

\section*{Probability of False Identifications}

There are \ac{at least }two \ac{contributors} to a false positive rate: coincidental matches (that is, two pieces of evidence that happen to have similar characteristics but are not from the same source) and human/technical failures \citep[p 50]{pcast}.
In objective disciplines with matching rules, such as DNA, these sources can be distinguished, and when coincidental match rates are very low, human/technical failures make up a large component of the error rate.
However, in toolmark examination, we do not have an objective matching rule; examiners testify based on subjective, individual rules for how much similarity is enough for an identification.
As a result, we cannot separate coincidental match rates from human/technical failures.
<<fpr, include = F>>=
errors <- c(20, 56, 1)
knm <- c(2842, 774, 220)

fpr <- sum(errors)/sum(knm)
@

%Compounding this problem, o
Open-set studies of striated comparisons (bullets and firing pin impressions) \citep{bajic2020, mattijssen2021,best2022} suggest \svp{that a starting point for estimating the} false positive rate for these comparisons is between 0.0045 \citep{best2022} and 0.072 \citep{mattijssen2021}; pooling data from these three studies yields a false positive rate of \Sexpr{sprintf("%.2f", fpr)}.

If we begin by considering the multiple comparison problem relative to the $\ell/d$ independent comparisons between a blade cut and a wire, we can compute the overall false positive rate for all comparisons using $r$, the false positive rate for one comparison, and simple probability \ac{calculations}. Let $$F_n:=\{\text{at least one false pos. in }n\text{ disjoint comparisons}\}.$$ Then, $P(F_1) = r$, $P(\overline{F_1}) = 1-r$, $P(F_n) = [1-r]^n$, and $P(\overline{F_n}) = 1 - \left[P(\overline{F_1})\right]^n$.
% Using this relationship between $F_n$ and the false positive rate $r$, we can determine what $r$ must be in order to limit $P(F_n)$ to an acceptable level by solving for $r$: $r = 1 - \left[1 - P(F_n)\right]^{1/n}$.
\svp{\Cref{fig:Accuracy} shows $P(\overline{F_n})$ on the y-axis; if we set 90\% as the acceptable} \ac{examination-wide false positive rate (FPR)}, we can make 2 comparisons when the single-comparison FPR is 0.05 and 10 comparisons at 0.01. Thus, at an error rate of \Sexpr{sprintf("%.2f", fpr)}, we can conduct approximately 5 comparisons \ac{in the course of an examination without exceeding the examination-wide FPR of 10\%.}

\svp{It is clear that under these constraints, the} accuracy of the \ac{forensic evaluation that involves multiple comparisons} between a wire and a tool \ac{is likely to be low}, as the number of candidate alignments that must be examined is too high.
Even the most innocuous example we could come up with (small blade, only 2 cutting surfaces, and a relatively large wire) involved a minimum of \Sexpr{2*ncomp_min} completely independent comparisons.
As a result, \ac{findings from}  wire comparisons made under current protocols (using error rates recently published in the forensics literature) \ac{are probably not} reliable enough to be presented at trial.
When we then consider that examiners would make cuts at multiple angles \citep{baikerToolmarkVariabilityQuality2015}, increasing the number of comparisons even in the simple case we have presented here, the probability of a false positive error increases further still.

<<control-overall-fpr, fig.cap="Exploration of overall error rate estimates for different values of $r$, the false positive rate for a single comparison. Dotted vertical lines show false positive error rate estimates (from left to right, \\citep{best2022,bajic2020,mattijssen2021}).", fig.width = 8, fig.height = 4, include = F>>=
library(ggplot2)
library(tidyr)
library(dplyr)
library(latex2exp)
library(geomtextpath)
single_rmp = exp(seq(-1000, 0, .1))
N = c(10, 100, 1000, 10000)

res = crossing(single_rmp, N) %>%
  mutate(overall = 1 - (1 - single_rmp)^N)

studies <- tibble(single_rmp = c( 0.004545,0.00704, 0.0724), study = c("Best (2022)","Bajic (2020)",  "Mattijssen (2021)"), y = c(-0.025, .15, 0.6), hjust = c(0, 0, 1))
ggplot(res, aes(x = single_rmp, y = overall, group = factor(N), color = factor(N))) +
  coord_cartesian(xlim = c(0, 0.07), ylim = c(0, 1)) +
  ylab("Probability of a false positive\n(N comparisons)") +
  xlab("Probability of a false positive\n(1 comparison)") +
  ggtitle("Wire Comparison: Controlling Overall False Positive Rate") +
  geom_vline(xintercept = 0.02, linetype = "dashed", color = "grey40") +
  geom_vline(xintercept = studies$single_rmp, linetype = "51", color = "grey50") +
  geom_label(data = studies[1,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[1], vjust = 0) +
  geom_label(data = studies[2,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[2], vjust = 0) +
  geom_label(data = studies[3,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[3], vjust = 0) +
  annotate(geom = "label", x = 0.02, y = 0.1, label = "Pooled error rate \nused in this paper", color = "grey40", hjust=0) +
  scale_color_discrete("# comparisons  ") +
  geom_line(linewidth=1) +
  theme_bw() +
  theme(legend.position = c(1, 0),
        legend.justification = c(1, 0),
        legend.background = element_rect(color = "grey40", fill=alpha("white", 0.1))) +
  scale_y_continuous(breaks = seq(0, 1, 0.1), minor_breaks = seq(0, 1, .05)) +
  scale_x_continuous(breaks = c(0, 0.02, 0.04, 0.06, studies$single_rmp),
                     labels=c(0, 0.02, 0.04, 0.06, "0.004545", "\n0.00704","0.0724"))
#  geom_hline(yintercept = 0.1, colour="black")
@


<<Accuracy, fig.cap="Probability of accurate decisions for $N$ comparisons for different error rates.", fig.width = 8, fig.height = 4>>=
r_vals <- tibble(r = c(.0005, 0.001, 0.005, .01, 0.05),
                 rlab = sprintf("P(F[1])==%0.05f", r))
res2 <- crossing(r = r_vals$r,
                 N = c(seq(1, 11, .01),seq(11.1, 109.9, .1), seq(110, 1100, 1))) %>%
  mutate(overall = (1 - r)^N) %>%
  left_join(r_vals)


breakpt <- .9

error_res <- filter(res2, lead(overall, 1) <= breakpt)
res2 <- filter(res2, overall >= breakpt)

res2sum <- res2 %>% group_by(r) %>% filter(overall == min(overall))


ggplot(data = res2, aes(x = N, y = overall, color = factor(r), label = rlab)) +
  geom_textline(parse = T, gap = F, straight = F,
                padding = unit(1, "mm"), vjust = 0, linewidth = 1) +
  xlab("Number of comparisons") +
  scale_x_log10(
    breaks = c(1, 1000, 10000, res2sum$N),
    labels = c("1", "1000", "10000", round(res2sum$N)),
    minor_breaks = c((1:9)[-2],10*(3:9), 100*3:9, 1000*1:9)
  ) +
  geom_hline(yintercept = breakpt, linewidth = 1) +
#  geom_label(
#    data = res2sum,
#    aes(x = N, y = breakpt, label = paste0("N == ", round(N))),
#    parse = T, vjust = 1, hjust = 1) +
  geom_segment(data = res2sum,
     aes(x = N, xend=N, y = -Inf, yend=0.9, color=factor(r)),
     linewidth=0.5) +
  geom_line(data = error_res, linetype = 2, linewidth = 0.75, alpha =0.5) +
  ylab("1 - Probability of false positive\n(N comparisons)") +
  ggtitle("# Comparisons and Overall Accuracy") +
  scale_color_discrete("FP Rate for single comparison", guide = 'none') +
  theme_bw() +
  coord_cartesian(ylim = c(0.775, 1.0)) +
  theme(axis.text.y = element_text(hjust = 0.5, vjust = 0.5, angle = 90))

@


\section*{Discussion \& Conclusions}

\ac{Forensic practitioners often report the findings from their examinations in the form of a categorical conclusion reflecting a single decision. This can be misleading when the one decision relies on multiple comparisons which are not individually presented in reports or testimony.} In this short contribution, we have shown that the implicit comparisons performed during the forensic analysis of wire cuts have the potential to vastly increase the overall chance that at least one false positive error occurs.
%While often a forensic conclusion is presented as a single decision, underlying that decision are many more comparisons which are not discussed or reported upon.
\ac{Omission of the underlying} details is not necessarily \ac{due to} malice; rather, it results from protocols which focus on finding the best possible match between two items and then evaluating that match, \ac{and also from lack of familiarity with basic statistical ideas.}

In our example \ac{we highlight a simple} case where a wire \ac{ is cut using a two-sided blade}, but the findings apply to many other situations where \ac{a forensic evaluation involves} multiple comparisons against a set of accumulated information. \ac{Examples include a database search or a situation similar to the example where multiple comparisons of evidence against reference samples are conducted.}
%either in a database or in a series of test samples created during the examination process.
As a result, it is critically important that forensic \ac{practitioners become aware of the impact of carrying out multiple comparisons on their final conclusions}
%science grapple with this multiple comparison problem and
and that they modify lab procedures and testimony accordingly. \ac{We suggest three strategies to improve transparency of forensic examinations and to produce more reliable estimates of examination-wide error rates.}
%We have three primary suggestions for improvements targeted at making the multiple comparisons visible to everyone involved in the process of evidence evaluation.

First, examiners should report (or defense attorneys should request) the overall length or area of surfaces generated from blade cuts, test fires, and equivalent procedures intended to assess the marks made by a tool submitted for comparison with evidence. Examiners should also report the total consecutive length or area of the recovered evidence. These pieces of information will allow for calculations of minimum and maximum number of comparisons (as we have demonstrated in this paper), and will facilitate calculation of overall error rates.

Second, researchers in this field should conduct studies relating the length/area of comparison surface to the error rate. For instance, here, we have combined studies looking at bullet striations with studies on firing pin shear marks - the striated surfaces are of orders of magnitude different lengths, but they represent our best estimate of the error rate for striated comparisons.
This was necessary, because we could not find black-box studies of wire cuts.
These studies should be Type II studies as defined by Koehler \citep{koehlerIntuitiveErrorRate2017a} in that they should be designed to assess the actual error rate when examiners are making difficult comparisons.

Finally, when databases are used at any stage of the forensic evidence evaluation process (from suitability assessment and triage to reports which will be used at trial), the number of database items searched and the number of results returned must be reported. Furthermore, the number of results used for futher manual comparison should also be reported. For example, if a firearms examiner searches a local NIBIN database with 1000 entries, requests the 20 closest matches to her evidence, and then carries out a physical examination of five exemplars from the list of 20, all of those values should be clearly reported to enable calculation of the potential FPR.
This will help make the multiple comparison issue visible for everyone involved in evaluating the forensic evidence, from examiners themselves to jurors and judges.

%We believe that the multiple comparison problems we have discussed here can be overcome with research and changes in evidence assessment procedures, but the first step is making the number of comparisons performed more visible to everyone involved in the process.

% Note: please start your introduction without including the word ``Introduction'' as a section heading (except for math articles in the Physical Sciences section); this heading is implied in the first paragraph.

% \section*{Guide to using this template on Overleaf}
%
% Please note that whilst this template provides a preview of the typeset manuscript for submission, to help in this preparation, it will not necessarily be the final publication layout. For more detailed information please see the \href{ https://www.pnas.org/page/authors/format}{PNAS Information for Authors}.
%
% If you have a question while using this template on Overleaf, please use the help menu (``?'') on the top bar to search for \href{https://www.overleaf.com/help}{help and tutorials}. You can also \href{https://www.overleaf.com/contact}{contact the Overleaf support team} at any time with specific questions about your manuscript or feedback on the template.
%
% \subsection*{Author Affiliations}
%
% Include department, institution, and complete address, with the ZIP/postal code, for each author. Use lower case letters to match authors with institutions, as shown in the example. PNAS strongly encourages authors to supply an \href{https://orcid.org/}{ORCID identifier} for each author. Individual authors must link their ORCID account to their PNAS account at \href{http://www.pnascentral.org/}{www.pnascentral.org}. For proper authentication, authors must provide their ORCID at submission and are not permitted to add ORCIDs on proofs.
%
% \subsection*{Submitting Manuscripts}
%
% All authors must submit their articles at \href{http://www.pnascentral.org/cgi-bin/main.plex}{PNAScentral}. If you are using Overleaf to write your article, you can use the ``Submit to PNAS'' option in the top bar of the editor window.
%
% \subsection*{Format}
%
% Many authors find it useful to organize their manuscripts with the following order of sections: title, author line and affiliations, keywords, abstract, introduction, results, discussion, materials and methods, acknowledgments, and references. Other orders and headings are permitted.
%
% \subsection*{Manuscript Length}
%
% Brief Reports are limited to 3 pages, which is approximately 1,600 words (including the manuscript text, title page, abstract, and figure legends), and 15 references. Supporting information (SI) is limited to extended methods, essential supporting datasets, and videos (no additional tables or figures).
%
%
% \subsection*{References}
%
% References should be cited in numerical order as they appear in text; this will be done automatically via bibtex, e.g. \cite{belkin2002using} and \cite{berard1994embedding,coifman2005geometric,phdthesis,masterthesis}. All references cited in the main text should be included in the main manuscript file.
%
% \subsection*{Data Archival}
%
% PNAS must be able to archive the data essential to a published article. Where such archiving is not possible, deposition of data in public databases, such as GenBank, ArrayExpress, Protein Data Bank, Unidata, and others outlined in the \href{https://www.pnas.org/author-center/editorial-and-journal-policies#materials-and-data-availability}{Information for Authors}, is acceptable.
%
% \subsection*{Language-Editing Services}
% Prior to submission, authors who believe their manuscripts would benefit from professional editing are encouraged to use a language-editing service (see list at https://www.pnas.org/author-center/language-editing). PNAS does not take responsibility for or endorse these services, and their use has no bearing on acceptance of a manuscript for publication.
%
% \begin{figure}[t!]
% \centering
% \includegraphics[width=.8\linewidth]{frog}
% \caption{Placeholder image of a frog with a long example legend to show justification setting.}
% \label{fig:frog}
% \end{figure}
%
%
% \begin{SCfigure*}[\sidecaptionrelwidth][t]
% \centering
% \includegraphics[width=11.4cm,height=11.4cm]{frog}
% \caption{This legend would be placed at the side of the figure, rather than below it.}\label{fig:side}
% \end{SCfigure*}
%
% \begin{table}[t!]
% \centering
% \caption{Comparison of the fitted potential energy surfaces and ab initio benchmark electronic energy calculations}
% \begin{tabular}{lrrr}
% Species & CBS & CV & G3 \\
% \midrule
% 1. Acetaldehyde & 0.0 & 0.0 & 0.0 \\
% 2. Vinyl alcohol & 9.1 & 9.6 & 13.5 \\
% 3. Hydroxyethylidene & 50.8 & 51.2 & 54.0\\
% \bottomrule
% \end{tabular}
%
% \addtabletext{nomenclature for the TSs refers to the numbered species in the table.}
% \end{table}
%
%
% \subsection*{Digital Figures}
%
% EPS, high-resolution PDF, and PowerPoint are preferred formats for figures that will be used in the main manuscript. Authors may submit PRC or U3D files for 3D images; these must be accompanied by 2D representations in TIFF, EPS, or high-resolution PDF format. Color images must be in RGB (red, green, blue) mode. Include the font files for any text.
%
% Images must be provided at final size, preferably 1 column width (8.7cm). Figures wider than 1 column should be sized to 11.4cm or 17.8cm wide. Numbers, letters, and symbols should be no smaller than 6 points (2mm) and no larger than 12 points (6mm) after reduction and must be consistent.
%
% Figures and tables should be labelled and referenced in the standard way using the \verb|\label{}| and \verb|\ref{}| commands.
%
% Figure \ref{fig:frog} shows an example of how to insert a column-wide figure. To insert a figure wider than one column, please use the \verb|\begin{figure*}...\end{figure*}| environment. Figures wider than one column should be sized to 11.4 cm or 17.8 cm wide. Use \verb|\begin{SCfigure*}...\end{SCfigure*}| for a wide figure with side legends.
%
% \subsection*{Tables}
% Tables should be included in the main manuscript file and should not be uploaded separately.
%
% \subsection*{Single column equations}
%
% Authors may use 1- or 2-column equations in their article, according to their preference.
%
% To allow an equation to span both columns, use the \verb|\begin{figure*}...\end{figure*}| environment mentioned above for figures.
%
% Note that the use of the \verb|widetext| environment for equations is not recommended, and should not be used.
%
% \begin{figure*}[bt!]
% \begin{align*}
% (x+y)^3&=(x+y)(x+y)^2\\
%        &=(x+y)(x^2+2xy+y^2) \numberthis \label{eqn:example} \\
%        &=x^3+3x^2y+3xy^3+x^3.
% \end{align*}
% \end{figure*}
%
%
%
% \subsection*{Supporting Information (SI)}
%
% Authors are limited to the following types of SI: extended methods, datasets, videos, and 3D figures. Extended discussion is not permitted.
%
% \subsection*{SI Appendix}
%
% Supply extended materials and methods information in a separate SI Appendix PDF file. Include SI movie legends. SI will be published as provided by the authors; it will not be edited or composed. Supplementary figures and tables are not allowed.
%
%
% \subsubsection*{SI Datasets}
%
% Supply .xlsx, .csv, .txt, .rtf, or .pdf files. This file type will be published in raw format and will not be edited or composed.
%
%
% \subsubsection*{SI Movies}
%
% Supply Audio Video Interleave (avi), Quicktime (mov), Windows Media (wmv), animated GIF (gif), or MPEG files and and include a brief legend for each movie in the main manuscript file. All movies should be submitted at the desired reproduction size and length. Movies should be no more than 10 MB in size.
%
%
%
%
% \matmethods{Please describe your materials and methods here. This can be more than one paragraph, and may contain subsections and equations as required.
%
% \subsection*{Subsection for Method}
% Example text for subsection.
% }
%
% \showmatmethods{} % Display the Materials and Methods section

\acknow{This work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.}

\showacknow{} % Display the acknowledgments section

% \bibsplit[3]
% %Use \bibsplit to split the references from the body of the text. Value "[3]" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.)


% Bibliography
% \bibliography{pnas-sample}

\bibliography{../references}
\end{document}
