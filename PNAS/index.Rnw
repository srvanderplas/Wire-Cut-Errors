\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\usepackage{cleveref}
\usepackage[dvipsnames]{xcolor} % colors
\newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
\newcommand{\hh}[1]{{\textcolor{ForestGreen}{#1}}}
\newcommand{\ac}[1]{{\textcolor{Purple}{#1}}}
\templatetype{pnasbriefreport}
%\templatetype{pnasbriefreport} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Hidden Multiple Comparisons Increase Forensic Error Rates}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1,2]{Susan Vanderplas}
\author[b, c]{Alicia Carriquiry}
\author[b, c, 1]{Heike Hofmann}

\affil[a]{Statistics Department, University of Nebraska Lincoln. 350 Hardin Hall, 3310 Holdrege North Wing, Lincoln, NE 68503}
\affil[b]{Department of Statistics, Iowa State University. 1121 Snedecor Hall, 2438 Osborn Dr, Ames, IA 50011}
\affil[c]{Center for Statistics and Applications in Forensic Evidence. 195 Durham Center, 613 Morrill Road, Ames, Iowa 50011}

% Please give the surname of the lead author for the running footer
\leadauthor{Vanderplas}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{The authors have no competing interests to declare.}
\equalauthors{\textsuperscript{1}SVP (Author One) contributed equally to this work with HH (Author Two).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: susan.vanderplas@unl.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{Forensic Evidence $|$ Statistics $|$ Wire cuts $|$ Toolmark analysis}


\begin{abstract}
When wires are cut, the tool produces striations on the cut surfaced; as in other forms of forensic analysis, these striation marks are used to connect the evidence to the source that created them. Here, we argue that the practice of comparing two wire cut surfaces introduces complexities not present in better-investigated forensic examination  of toolmarks such as those observed on bullets, as wire comparisons inherently require multiple distinct comparisons, increasing the expected false positive rate. We call attention to the multiple comparison problem in wire examination and relate it to other situations in forensics that involve multiple comparisons, such as database searches.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}
<<setup, include = F>>=
library(knitr)
opts_chunk$set(error=F, warning=F, message = F, dpi=300, echo = F)
options(digits = 4)
@

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}


\firstpage[5]{5}

\dropcap{I}n forensic evaluations, a single conclusion often relies on  many comparisons, either implicitly or explicitly.
Multiple comparisons arise persistently when developing statistical methods to address scientific problems \citep{benjaminiControllingFalseDiscovery1995}, and greatly increase the probability of false discoveries.
Now that vast databases and efficient algorithms are routinely used in forensic evaluations to propose matches to crime scene items, the problem of close non-matches~\citep{pcast}  due to multiple comparisons becomes critically important.
Scientists have the responsibility to address this often ignored issue to prevent a further erosion of the public's confidence in the justice system.
The multiple comparison problem is not new: it has been raised in the past with regard to DNA~\citep{thompson2003} and latent print evaluations~ \citep{koehler2021}. One of the root causes~\citep{fine2006} leading to the wrongful accusation of Brandon Mayfield in the 2004 Madrid train bombing case was that the large size of the IAFIS database used to search for similar prints made it possible to locate `unusually' close non-matches.
The probability of finding  close non-matches for an item of evidence is proportional to the size of the database available for searching; close non-matches occur more frequently as databases increase in size and algorithms become more powerful.

Compounding this issue, the use of algorithms also results in a large number of comparisons that are not obvious to the user.
For example, the cross-correlation function~\citep{vorburgerApplicationsCrosscorrelationFunctions2011} was one of the first measures proposed for quantifying the similarity between two patterns in response to the 2009 NRC report~\citep{nas2009}, and continues to be used in many pattern searching algorithms to find the best alignment between two images and to quantify their overall similarity.
Finding the best alignment often consists in sliding one surface across the whole length (for one-dimensional patterns, such as striations) or area (for two dimensional sources, such as impression marks) of the other item while keeping track of the value of a similarity measure.
This mirrors the forensic examination process: the examiner visually rotates and shifts items under a comparison microscope to align two surfaces.
In order to avoid false accusations and further erosions of public trust in science, we must address the problem of multiple comparisons and control their effect on false discovery rates.

Here, we consider the multiple comparisons problem that arises from a relatively simple toolmark examination: matching a cut wire to a wire-cutting tool.
We describe the comparison approach, estimate the (minimal) number of comparisons that are needed to carry out the examination, and discuss how the false discovery rate changes with the number of comparisons involved, using error rates derived from published black-box studies.


\section*{Examination Process}

A forensics examiner tasked with determining whether a wire in evidence was cut by a specific tool will create one or more blade cuts using the putative tool, which are then compared to the cut surface of the wire.
These cuts are made in a sheet of material similar to the wire's composition, and may be performed at multiple angles, as the angle of the tool to the substrate can affect which striations are recorded on the substrate surface.
The blade cuts will then be compared to the wire under a comparison microscope, though eventually, automatic comparison algorithms may also be validated for lab use.
Each side of each blade cut will be compared to each side of the wire; depending on the tool design, there can be between two and four cutting surfaces in contact with the substrate.

\section*{Calculating the Number of Comparisons}

In order to calculate the number of comparisons carried out in the course of one examination, we define $\ell$ to be the length of the blade cut, and $d$ to be the diameter of the wire.
We assume that the wire is covered with striations suitable for comparison across its full diameter $d$.
If this is not the case, we reduce the value $d$.
Both the blade and the wire are either digitally scanned at resolution $r$ mm per pixel, or visually examined using a microscope with a digital resolution that can be expressed as $r$ equivalent to the digital scan.
An illustration of the sliding comparison process is shown in \Cref{fig:wire-blade}.
Imagine that we move the cut wire along the blade cut in order to assess whether striations on the blade cut match the striations on the wire.
We can move the wire unit-by-unit, or we can move the wire by its full length, with no overlap to the previous comparison.

The first option gives us the maximum number of comparisons ($\ell/r - d/r + 1$), while the second option gives us the minimum number of comparisons $\ell/d$.
In the first case, sequential comparisons share much of the same physical data and are highly related; in the second case, no data is shared between physical comparisons and we can expect that they are statistically independent, though empirically there will be nonzero correlations due to physical similarities between striations.
For simplicity, let us consider the number of comparisons to lie somewhere between these two estimates.
Note that when $\ell/d \approx 1$, as in bullet LEA comparisons, the minimal and the maximal number of comparisons are both close to 1.
Finally, we must consider the number of surfaces which must be compared: the wire may have between one and two sets of striae and there may be 2-4 different blade cut surfaces to examine, depending on tool configuration.
This results in a multiplier of between 2 and 8 to calculate the total number of comparisons required.

\subsection*{A concrete example}

<<simple-problem-setup, include=F>>=
ell <- 15
d <- 2
res <- 0.645/1000
ncomp_min <- ell/d
ncomp_max <- ell/res - d/res + 1

# From 3 studies
errors <- c(20, 56, 1)
knm <- c(2842, 774, 220)

fpr <- sum(errors)/sum(knm)
@

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{../fig/wire-blade-comparison-label.png}
\includegraphics[width=\columnwidth]{../fig/wire_cutter_wires.png}
\caption{(Top) A comparison between a wire and a blade cut requires sliding the wire along the entire blade cut length to determine the best match (or whether there is a match). Surfaces shown are rendered 2D topographical scans of a wire and blade cut taken with a confocal light microscope. (Bottom) RJ45 Crimp tool with a $\Sexpr{ell/10}$ cm razor blade used for cutting. 1 mm and 2 mm diameter aluminum wires cut with the pliers are shown in a box in the top right corner.}\label{fig:tool-pic}\label{fig:wire-blade}
\end{figure}

Let us consider a wire-cutting tool with a $\Sexpr{ell/10}$ cm razor blade that meets a cast surface (one such tool is shown in \Cref{fig:tool-pic}); the wire is held against this rectangular cast surface as the blade is pushed into the wire, splitting it in two.
This is a minimal scenario - the wire will acquire striations from one side of the blade, while the blade itself has two cutting edges, which we will call side A and side B.
A blade cut of a sheet of aluminum will thus produce two striated edges corresponding to side A and side B which are compared to cut wires to assess similarity.
We also have a 12 gauge aluminum wire ($\Sexpr{d}$ mm diameter) which may have been cut by the wire-cutting tool described above.
Class characteristics, which are shared by all tools of similar manufacture, appear to match:
there is a flat impression on one side of the wire corresponding to the cast metal backstop of the tool,
and the wire is cut such that the blade and the backstop appear to be perpendicular
(that is, the wire appears to have been cut with a tool of similar configuration).
In this example, $\ell = \Sexpr{ell}$ mm, $d = \Sexpr{d}$ mm, and there are at least $\ell/d = \Sexpr{ncomp_min}$ comparisons between a wire cut and a blade cut.
As there are two blade cuts (side A and side B), the minimal number of comparisons overall is \Sexpr{ncomp_min*2}, as these comparisons are non-overlapping and independent (on average).

Assuming a resolution of $\Sexpr{res*1000} \mu$m per pixel, the maximum number of comparisons per blade cut is around $\Sexpr{format(round(ncomp_max/1000)*1000, big.mark=',')}$; thus, we need $\Sexpr{format(round(ncomp_max/1000)*2000, big.mark=',')}$ comparisons overall in order to find the optimal alignment between the wire and the blade cut.
These comparisons are implicit in the calculation of cross-correlation, which is the first and often the only step used to quantitatively assess the similarity between striated evidence such as bullets, aperture shear, and firing pin impressions.
Implicit comparisons are not unique to algorithms; an examiner would need to physically align the wire and the blade cut by searching along the length of the cut to visually match striations, performing the same process physically that the algorithm performs computationally.
While these sequential comparisons are highly auto correlated, and we cannot assume sequential independence when calculating the probability of an error, they serve as an upper bound on the number of comparisons which could be performed.
As the number of comparisons increases, the probability of encountering a coincidental match increases.
Statisticians call this the \textit{family-wise error rate} $E$; it is an important quantity to control when conducting a series ("family") of tests.


\section*{Probability of False Identifications}
There are at least two components of the false positive rate (FPR): identifying two pieces of evidence that have similar characteristics but are from different sources (a coincidental match) and procedural failures (e.g.\ lab process errors) \citep[p 50]{pcast}.
In objective disciplines with standardized evaluation rules (e.g.\ DNA), these sources can be distinguished.
However, in toolmark examination, no objective evaluation rules are used; examiners testify based on subjective, individual rules for how much similarity is sufficient for an identification.
Assuming that lab procedure errors are not a factor in studies, we use reported error rates from open-set studies of striated evidence comparisons to estimate the coincidental match rate of a single wire-cut comparison.
The three studies we consider \citep{bajic2020, mattijssen2021,best2022} have FPRs between 0.0045 \citep{best2022} and 0.072 \citep{mattijssen2021}; pooling data from these studies yields a FPR of \Sexpr{sprintf("%.2f", fpr)}.
For a single-comparison FPR of $e$, the family-wise FPR for $n$ comparisons, $E_n$ is $1 - [1-e]^n$.
\Cref{fig:Accuracy} shows the family-wise accuracy rate on the y-axis; if 90\% is an acceptable overall accuracy, 2 comparisons can be made when $e = 0.05$; increasing to 10 comparisons when $e =  0.01$.
Thus, at an error rate of \Sexpr{sprintf("e = %.2f", fpr)}, examiners can conduct comparisons before exceeding the family-wise FPR of 10\%.

Under these constraints, the accuracy of an examination involving multiple comparisons between a wire and a tool will be low, as the number of candidate alignments that must be examined is high.
Even the most innocuous example (small blade, only 2 cutting surfaces, and a relatively large wire) involved a minimum of \Sexpr{2*ncomp_min} completely independent comparisons.
When we then consider that examiners would make cuts at multiple angles \citep{baikerToolmarkVariabilityQuality2015}, increasing the number of comparisons, the probability of a false identification becomes even more likely.
As a result, it is questionable whether wire comparisons made under current protocols are reliable enough to be presented at trial.

<<control-overall-fpr, fig.cap="Exploration of overall error rate estimates for different values of $e$, the FPR for a single comparison. Dotted vertical lines show false positive error rate estimates (from left to right, \\citep{best2022,bajic2020,mattijssen2021}).", fig.width = 8, fig.height = 4, include = F>>=
library(ggplot2)
library(tidyr)
library(dplyr)
# library(latex2exp)
library(geomtextpath)
single_rmp = exp(seq(-1000, 0, .1))
N = c(10, 100, 1000, 10000)

res = crossing(single_rmp, N) %>%
  mutate(overall = 1 - (1 - single_rmp)^N)

studies <- tibble(single_rmp = c( 0.004545,0.00704, 0.0724), study = c("Best (2022)","Bajic (2020)",  "Mattijssen (2021)"), y = c(-0.025, .15, 0.6), hjust = c(0, 0, 1))
ggplot(res, aes(x = single_rmp, y = overall, group = factor(N), color = factor(N))) +
  coord_cartesian(xlim = c(0, 0.07), ylim = c(0, 1)) +
  ylab("Probability of a false positive\n(N comparisons)") +
  xlab("Probability of a false positive\n(1 comparison)") +
  ggtitle("Wire Comparison: Controlling Overall False Positive Rate") +
  geom_vline(xintercept = 0.02, linetype = "dashed", color = "grey40") +
  geom_vline(xintercept = studies$single_rmp, linetype = "51", color = "grey50") +
  geom_label(data = studies[1,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[1], vjust = 0) +
  geom_label(data = studies[2,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[2], vjust = 0) +
  geom_label(data = studies[3,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[3], vjust = 0) +
  annotate(geom = "label", x = 0.02, y = 0.1, label = "Pooled error rate \nused in this paper", color = "grey40", hjust=0) +
  scale_color_discrete("# comparisons  ") +
  geom_line(linewidth=1) +
  theme_bw() +
  theme(legend.position = c(1, 0),
        legend.justification = c(1, 0),
        legend.background = element_rect(color = "grey40", fill=alpha("white", 0.1))) +
  scale_y_continuous(breaks = seq(0, 1, 0.1), minor_breaks = seq(0, 1, .05)) +
  scale_x_continuous(breaks = c(0, 0.02, 0.04, 0.06, studies$single_rmp),
                     labels=c(0, 0.02, 0.04, 0.06, "0.004545", "\n0.00704","0.0724"))
#  geom_hline(yintercept = 0.1, colour="black")
@


<<Accuracy, fig.cap="Probability of accurate decisions for $N$ comparisons for different error rates.", fig.width = 8, fig.height = 4>>=
r_vals <- tibble(r = c(.0005, 0.001, 0.005, .01, 0.05),
                 rlab = sprintf("P(F[1])==%0.05f", r))
res2 <- crossing(r = r_vals$r,
                 N = c(seq(1, 11, .01),seq(11.1, 109.9, .1), seq(110, 1100, 1))) %>%
  mutate(overall = (1 - r)^N) %>%
  left_join(r_vals)


breakpt <- .9

error_res <- filter(res2, lead(overall, 1) <= breakpt)
res2 <- filter(res2, overall >= breakpt)

res2sum <- res2 %>% group_by(r) %>% filter(overall == min(overall))


ggplot(data = res2, aes(x = N, y = overall, color = factor(r), label = rlab)) +
  geom_textline(parse = T, gap = F, straight = F,
                padding = unit(1, "mm"), vjust = 0, linewidth = 1) +
  xlab("Number of comparisons") +
  scale_x_log10(
    breaks = c(1, 1000, 10000, res2sum$N),
    labels = c("1", "1000", "10000", round(res2sum$N)),
    minor_breaks = c((1:9)[-2],10*(3:9), 100*3:9, 1000*1:9)
  ) +
  geom_hline(yintercept = breakpt, linewidth = 1) +
#  geom_label(
#    data = res2sum,
#    aes(x = N, y = breakpt, label = paste0("N == ", round(N))),
#    parse = T, vjust = 1, hjust = 1) +
  geom_segment(data = res2sum,
     aes(x = N, xend=N, y = -Inf, yend=0.9, color=factor(r)),
     linewidth=0.5) +
  geom_line(data = error_res, linetype = 2, linewidth = 0.75, alpha =0.5) +
  ylab("1 - Probability of false positive\n(N comparisons)") +
  ggtitle("# Comparisons and Overall Accuracy") +
  scale_color_discrete("FP Rate for single comparison", guide = 'none') +
  theme_bw() +
  coord_cartesian(ylim = c(0.775, 1.0)) +
  theme(axis.text.y = element_text(hjust = 0.5, vjust = 0.5, angle = 90))

@


\section*{Discussion \& Conclusions}

Forensic practitioners often report the findings from their examinations in the form of a categorical conclusion reflecting a single decision.
This is misleading when the decision relies on multiple comparisons which are not individually presented in reports or testimony.
In this short contribution, we have shown that the implicit comparisons performed during forensic analysis of wire cuts increase the family-wise error rate.
Omission of the underlying details is not necessarily due to malice; rather, it results from protocols which focus on finding the best possible match between two items and evaluating that match.

In our example we highlight a simple case where a wire is cut using a two-sided blade, but the findings apply to many other situations where a forensic evaluation involves multiple comparisons against a set of accumulated information, such as database searches.
It is critically important that forensic practitioners become aware of the impact of carrying out multiple comparisons on their final conclusions and that lab procedures and testimony are modified accordingly.
We suggest three strategies to improve transparency of forensic examinations and to produce more reliable estimates of examination-wide error rates.

First, examiners should report (or defense attorneys should request) the overall length or area of surfaces generated during the examination process, along with the total consecutive length or area of the recovered evidence.
These pieces of information will take the place of $\ell$ and $d$ and facilitate calculation of examination-wide error rates.

Second, researchers should conduct studies relating the length/area of comparison surface to the error rate.
For instance, we have pooled studies looking at bullet striations and firing pin shear marks because we could not find black-box error rate studies of wire cuts.
The striated surfaces are of orders of magnitude different lengths, but represent the best estimate of the error rate for striated materials.
New studies should be Type II studies as defined by Koehler \citep{koehlerIntuitiveErrorRate2017a}, designed to assess the actual error rate when examiners are making difficult comparisons.

Finally, when databases are used at any stage of the forensic evidence evaluation process (from suitability assessment and triage to reports which will be used at trial), the number of database items searched (or comparisons made) and the number of results returned must be reported.
Additionally, the number of results used for further manual comparison should also be reported.
For example, if a firearms examiner searches a local NIBIN database with 1000 entries, requests the 20 closest matches to her evidence, and then carries out a physical examination of five exemplars from the list of 20, all of those values should be clearly reported to enable estimation of the examination-wise error rate.
This will help make the multiple comparison issue accessible to everyone involved in evaluating the value of forensic evidence, from examiners to lawyers to jurors and judges.


\acknow{This work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.}

\showacknow{} % Display the acknowledgments section

\bibliography{../references}
\end{document}
