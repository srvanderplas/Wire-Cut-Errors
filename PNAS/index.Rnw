\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\usepackage{cleveref}
\usepackage[dvipsnames]{xcolor} % colors
\newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
\newcommand{\hh}[1]{{\textcolor{ForestGreen}{#1}}}
\templatetype{pnasbriefreport} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Template for preparing your Brief Report submission to PNAS using Overleaf}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1,2]{Susan Vanderplas}
\author[b, c]{Alicia Carriquiry}
\author[b, c, 1]{Heike Hofmann}

\affil[a]{Statistics Department, University of Nebraska Lincoln. 350 Hardin Hall, 3310 Holdrege North Wing, Lincoln, NE 68503}
\affil[b]{Department of Statistics, Iowa State University. 1121 Snedecor Hall, 2438 Osborn Dr, Ames, IA 50011}
\affil[c]{Center for Statistics and Applications in Forensic Evidence. 195 Durham Center, 613 Morrill Road, Ames, Iowa 50011}

% Please give the surname of the lead author for the running footer
\leadauthor{Vanderplas}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{The authors have no competing interests to declare.}
\equalauthors{\textsuperscript{1}SVP (Author One) contributed equally to this work with HH (Author Two).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: susan.vanderplas@unl.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{Forensic Evidence $|$ Statistics $|$ Wire cuts $|$ Toolmark analysis}


\begin{abstract}
Please provide an abstract of no more than 250 words in a single paragraph. Abstracts should explain to the general reader the major contributions of the article. References in the abstract must be cited in full within the abstract itself and cited in the text.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}
<<setup, include = F>>=
library(knitr)
opts_chunk$set(error=F, warning=F, message = F, dpi=300, echo = F)
@

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\firstpage[3]{8}
% Use \firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. "\firstpage[4]{11}".


% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
% \dropcap{T}his PNAS journal template is provided to help you write your work in the correct journal format. Instructions for use are provided below.

\dropcap{M}ultiple comparisons are a persistent problem when developing statistical methods for scientific problems \citep{benjaminiControllingFalseDiscovery1995}. In forensic statistics, researchers have been focusing on the necessity of building up research databases which can be used to develop objective algorithms for evidence comparisons and explore closest non-matches \citep{pcast}. While these databases (and open data) are critically important in forensics, we also need to be careful about how our methods are affected by the multiple comparison problem.

As we develop objective algorithms and standardize laboratory procedures in forensic pattern evidence, we must remain cognizant that a single conclusion is often based on  many comparisons, either implicitly or explicitly. \hh{Algorithms, in particular, make a large number of comparisons out of a user's sight. The cross-correlation function, for example, has been suggested \citep{vorburgerApplicationsCrosscorrelationFunctions2011} as one of the first quantitative measures in response to the NAS report\citep{nas2009}, and is used in most pattern searching algorithms to find the best alignment between two sources and also evaluate the overall similarity. }
\hh{Finding the best} \svp{alignment} \hh{requires implicitly} \svp{sliding} \hh{one source across the whole length (for one-dimensional sources, such as striations) or area (for two dimensional sources, such as impression marks) of the other source and} \svp{keeping} \hh{track of the similarity measure.}
\svp{This mirrors the forensic examination process, as examiners visually}
rotate or align two pieces of evidence under comparison microscopes.

An explicit search, in constrast, occurs when we conduct database searches.
These searches often return the $N$ best matches from the database, but this requires making comparisons to all of the database entries which meet the search criteria.

The multiple comparison problems in forensics are distinct from the familywise error rate and false discovery rate problems that have been previously addressed in major ways: our tests are sequential, not concurrent, with dependencies depending on both the testing procedure and the data. In addition, an identification might be sufficient to end the search; as a result, this problem is more akin to a ``search until you find" problem than a situation where a family-wise error rate or false discovery rate is applicable - we are interested in the probability of at least one error in a series of sequential comparisons.

The multiple testing problem is closely related to issues which have caused embarrassing false accusations and mistrust of forensic pattern evidence: partial matches and database searches. As we assemble research databases for explicitly making many comparisons and tackle problems which implicitly involve multiple comparisons, we must confront these issues before more embarrassing missteps lead the public to question the validity of forensic science.

In this paper, we consider the multiple comparisons problem that arises from a relatively simple toolmark comparison: matching a cut wire to a wire-cutting tool. We describe the approach and calculate the number of comparisons performed, and then assess the likelihood of a false positive error considering the multiple comparisons performed, using error rates derived from published black-box studies.

\section*{Examination Process}

A forensics examiner tasked with determining whether a wire in evidence was cut by a specific tool will create one or more blade cuts which can be compared to the cut surface of the wire. These cuts are made in a sheet of material similar to the wire's composition, and may be performed at multiple angles, as the angle of the tool to the substrate can affect which striations are recorded on the substrate surface. The blade cuts will then be compared to the wire under a comparison microscope; eventually, automatic comparison algorithms may also be validated for lab use. Each side of each blade cut will be compared to each side of the wire; depending on the tool design, there can be between 2 and 4 cutting surfaces in contact with the substrate.

\section*{Calculating the Number of Comparisons}

In order to calculate the number of comparisons, we define $\ell$ to be the length of the blade cut, and $d$ to be the diameter of the wire. Without loss of generality, we will assume that the wire is fully covered with striations suitable for comparison; if this is not the case, it is equivalent to changing the value of $d$. At minimum, there must be $\ell/d$ comparisons in order to assess whether one side of the blade cut matches one side of the wire; in this case, the comparisons are independent of each other (conditional on striation patterns) because no overlapping data is used. Of course, this overlooks the possibility of partial matches and assumes perfect alignment between the diameter of the wire and the portion of the blade used to cut the wire, which is unrealistic.

At a maximum, both the blade and the wire are either digitally scanned at resolution $r$ mm, or visually examined using a microscope with a digital resolution that can be expressed as $r$ equivalent to the digital scan. In this case, there may be as many as $\ell/r - d/r + 1$ full comparisons performed in order to find the optimal alignment between the two samples. These comparisons are of course highly dependent, as each sequential comparison shares much of the same data with the previous and next comparisons; such dependency could be expressed via autocorrelation, but we will defer that derivation to a later paper. In this paper, let us consider the number of comparisons to lie somewhere between the minimum and maximum number of comparisons.

Then, we must consider the number of surfaces which must be compared: the wire may have between one and two sets of striae (depending on tool configuration) and there may be between 2 and 4 different blade cut surfaces to examine. This results in a multiplier of between 2 and 8 comparisons to be conducted to compare the striations on the wire to the striations on the blade cuts, even before we consider the problem of cutting angles.

\subsection*{A concrete example}
Let us consider a wire-cutting tool with a 1.5 cm razor blade that meets a cast surface; the wire is held against this rectangular cast surface as the blade is pushed into the wire, splitting it in two. This is a minimal scenario - the wire will have striations from one side of the blade, while the blade itself has two cutting edges, which we will call side A and side B. The blade cuts of a sheet of aluminum that is 1.5 cm in width thus produce two striated edges corresponding to side A and side B which can be compared to other cuts to assess similarity.

In addition, we have a 12 gauge aluminum wire (2 mm diameter) which may have been cut  by the wire-cutting tool described above. Class characteristics, which are shared by all tools of similar manufacture, appear to match: there is a flat impression on one side of the wire corresponding to the cast metal backstop of the tool, and the wire is cut such that the blade and the backstop appear to be perpendicular (that is, the wire appears to have been cut with a blade similar to the blade configuration of the tool in question).

<<simple-problem-setup, echo = F, warning = F, message = F>>=
ell <- 15
d <- 2
res <- 0.645/1000
ncomp_min <- ell/d
ncomp_max <- ell/res - d/res + 1
@


In this example, $\ell = \Sexpr{ell} mm$, $d = \Sexpr{2} mm$, and the minimal number of comparisons between the wire and one blade cut is $\ell/d = \Sexpr{ncomp_min}$. As there are two blade cuts (side A and side B), the minimal number of comparisons overall is \Sexpr{ncomp_min*2}, and these comparisons are independent.

If the blade cuts and the wire edge are scanned at a resolution of $\Sexpr{res*1000} \mu m$, the resulting digital files will require $\Sexpr{ncomp_max}$ comparisons per blade cut, or $\Sexpr{ncomp_max*2}$ comparisons overall. These comparisons are required in order to find the optimal alignment between the wire and the blade cut; they are implicit in the calculation of cross-correlation which is either the first step or the only step used to objectively assess the similarity between striated evidence such as bullets, aperture shear, and firing pin impressions. This problem is not unique to algorithms, however; and examiner would need to physically align the wire and the blade cut by searching along the length of the cut to visually match striations, performing the same process physically that the algorithm performs computationally. While these sequential comparisons are of course highly autocorrelated, and we cannot assume sequential independence when calculating the probability of an error, they serve as an upper bound on the number of comparisons which could be performed.

\section*{Probability of False Identifications}

There are two components to a false positive rate: coincidental matches (that is, two pieces of evidence that happen to have similar characteristics but are not from the same source) and human/technical failures \citep[p 50]{pcast}. In objective disciplines with matching rules, such as DNA, these sources can be distinguished, and when coincidental match rates are very low, human/technical failures make up a large component of the error rate. However, in toolmark examination, we do not have an objective matching rule; examiners testify based on subjective, individual rules for how much similarity is enough for an identification. As a result, we cannot separate coincidental match rates from human/technical failures.
<<fpr, include = F>>=
errors <- c(20, 56, 1)
knm <- c(2842, 774, 220)

fpr <- sum(errors)/sum(knm)
@

Compounding this problem, open-set studies of striated comparisons (bullets, firing pin impressions) \citep{bajic2020, mattijssen2021,best2022} suggest the false positive rate for these comparisons is between 0.004545 \citep{best2022} and 0.07235 \citep{mattijssen2021}; pooling data from these three studies yields a false positive rate of is \Sexpr{sprintf("%.2f", fpr)}. %, which is high relative to the consequences of a false identification of evidence from different sources.

If we begin by considering the multiple comparison problem relative to the $\ell/d$ independent comparisons between a blade cut and a wire, we can calculate the overall false positive rate for all comparisons using $r$, the false positive rate for one comparison, and simple probability. Let $F_n:=\{\text{at least one false positive in }n\text{ comparisons}\}$. Then,
\begin{align}
P(F_1) &= r\nonumber\\
P(\overline{F_1}) &= 1-r\nonumber\\
P(F_n) &= \left[1-r\right]^n\nonumber\\
P(\overline{F_n}) &= 1 - \left[P(\overline{F_1})\right]^n = 1 - \left[1 - r\right]^n.\label{eq:fpr-control}
\end{align}
Using this relationship between $F_n$ and the false positive rate $r$, we can determine what $r$ must be in order to limit $P(F_n)$ to an acceptable level by solving for $r$: $r = 1 - \left[1 - P(F_n)\right]^{1/n}$.

\Cref{fig:control-overall-fpr} explores the implications of this relationship.
The astonishing conclusion is that even at the smallest false positive error rate for striated comparisons, we have about a 3\% chance of at least one false positive conclusion when 10 comparisons are made and about a 37.5\% chance of at least one false positive conclusion when 100 comparisons are made.
These results suggest that using current estimates of false positive error rates, it is nearly impossible for wire comparisons to provide compelling, 'beyond a reasonable doubt' levels of evidence.

<<control-overall-fpr, fig.cap="Exploration of overall error rate estimates for different values of $r$, the false positive rate for a single comparison. Dotted vertical lines show false positive error rate estimates (from left to right, \\citep{best2022,bajic2020,mattijssen2021}).", fig.width = 8, fig.height = 4>>=
library(ggplot2)
library(tidyr)
library(dplyr)
library(latex2exp)
library(geomtextpath)
single_rmp = exp(seq(-1000, 0, .1))
N = c(10, 100, 1000, 10000)

res = crossing(single_rmp, N) %>%
  mutate(overall = 1 - (1 - single_rmp)^N)

studies <- tibble(single_rmp = c( 0.004545,0.00704, 0.0724), study = c("Best (2022)","Bajic (2020)",  "Mattijssen (2021)"), y = c(-0.025, .15, 0.6), hjust = c(0, 0, 1))
ggplot(res, aes(x = single_rmp, y = overall, group = factor(N), color = factor(N))) +
  geom_line() +
  coord_cartesian(xlim = c(0, 0.07), ylim = c(0, 1)) +
  ylab("Probability of a coincidental match (N comparisons)") +
  xlab("Probability of a false positive (1 comparison)") +
  scale_y_continuous(breaks = seq(0, 1, .1), minor_breaks = seq(0, 1, .05)) +
  ggtitle("Wire Comparison: Controlling Overall False Positive Rate") +
  geom_vline(xintercept = 0.02, linetype = "dashed", color = "grey40") +
  geom_vline(xintercept = studies$single_rmp, linetype = "51", color = "grey50") +
  geom_label(data = studies[1,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[1], vjust = 0) +
  geom_label(data = studies[2,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[2], vjust = 0) +
  geom_label(data = studies[3,], aes(x = single_rmp, y = y, label = study), inherit.aes = F, hjust = studies$hjust[3], vjust = 0) +
  annotate(geom = "label", x = 0.02, y = 0.1, label = "Pooled error rate \nused in this paper", color = "grey40", hjust=0) +
  scale_color_discrete("# comparisons") +
  theme_bw() +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0), legend.background = element_rect(color = "black"))
@

If we look at this data differently, we can assess the probability that $N$ comparisons are made accurately (e.g. there are no coincidental matches or examiner errors).
It is clear from \Cref{fig:Accuracy} that with existing false positive error rate estimates between 0.07 and 0.0045, the accuracy of the aggregate comparison between a wire and a tool becomes questionable, as the number of candidate alignments that must be examined is too high.
Even the most innocuous example we could come up with (small blade, only 2 cutting surfaces, and a relatively large wire) involved a minimum of \Sexpr{2*ncomp_min} completely independent comparisons.
As a result, it seems improbable that a wire comparison made under current protocols (with corresponding error rates) would have an overall accuracy rate which would be reliable enough to be presented at trial.
When we then consider that examiners would make cuts at multiple angles \citep{baikerToolmarkVariabilityQuality2015}, increasing the number of comparisons even in the simple case we have presented here, the probability of a coincidental match increases

<<Accuracy, fig.cap="Probability of accurate decisions for $N$ comparisons for different error rates.", fig.width = 8, fig.height = 4>>=
r_vals <- tibble(r = c(.0005, 0.001, 0.005, .01, 0.05),
                 rlab = sprintf("P(F[1])==%0.05f", r))
res2 <- crossing(r = r_vals$r,
                 N = c(seq(1, 11, .01),seq(11.1, 109.9, .1), seq(110, 1100, 1), seq(1110, 10000, 10))) %>%
  mutate(overall = (1 - r)^N) %>%
  left_join(r_vals)


breakpt <- .9

error_res <- filter(res2, lead(overall, 1) <= breakpt)
res2 <- filter(res2, overall >= breakpt)

res2sum <- res2 %>% group_by(r) %>% filter(overall == min(overall))


ggplot(data = res2, aes(x = N, y = overall, color = factor(r), label = rlab)) +
  geom_textline(parse = T, gap = F, straight = F, padding = unit(1, "mm"), vjust = 0, linewidth = 0.75) +
  geom_line(data = error_res, linetype = 2, linewidth = 0.75) +
  xlab("Number of comparisons") +
  scale_x_log10() +
  geom_hline(yintercept = breakpt, linewidth = 1) +
  geom_label(data = res2sum, aes(x = N, y = breakpt, label = paste0("N == ", round(N))), parse = T, vjust = 1, hjust = 1) +
  ylab("Probability of no coincidental matches (N comparisons)") +
  ggtitle("# Comparisons and Overall Accuracy") +
  scale_color_discrete("FP Rate for single comparison", guide = 'none') +
  theme_bw() +
  coord_cartesian(ylim = c(0.75, 1.0)) +
  theme(axis.text.y = element_text(hjust = 0.5, vjust = 0.5, angle = 90))

@


\section*{Discussion}



% Note: please start your introduction without including the word ``Introduction'' as a section heading (except for math articles in the Physical Sciences section); this heading is implied in the first paragraph.

% \section*{Guide to using this template on Overleaf}
%
% Please note that whilst this template provides a preview of the typeset manuscript for submission, to help in this preparation, it will not necessarily be the final publication layout. For more detailed information please see the \href{ https://www.pnas.org/page/authors/format}{PNAS Information for Authors}.
%
% If you have a question while using this template on Overleaf, please use the help menu (``?'') on the top bar to search for \href{https://www.overleaf.com/help}{help and tutorials}. You can also \href{https://www.overleaf.com/contact}{contact the Overleaf support team} at any time with specific questions about your manuscript or feedback on the template.
%
% \subsection*{Author Affiliations}
%
% Include department, institution, and complete address, with the ZIP/postal code, for each author. Use lower case letters to match authors with institutions, as shown in the example. PNAS strongly encourages authors to supply an \href{https://orcid.org/}{ORCID identifier} for each author. Individual authors must link their ORCID account to their PNAS account at \href{http://www.pnascentral.org/}{www.pnascentral.org}. For proper authentication, authors must provide their ORCID at submission and are not permitted to add ORCIDs on proofs.
%
% \subsection*{Submitting Manuscripts}
%
% All authors must submit their articles at \href{http://www.pnascentral.org/cgi-bin/main.plex}{PNAScentral}. If you are using Overleaf to write your article, you can use the ``Submit to PNAS'' option in the top bar of the editor window.
%
% \subsection*{Format}
%
% Many authors find it useful to organize their manuscripts with the following order of sections: title, author line and affiliations, keywords, abstract, introduction, results, discussion, materials and methods, acknowledgments, and references. Other orders and headings are permitted.
%
% \subsection*{Manuscript Length}
%
% Brief Reports are limited to 3 pages, which is approximately 1,600 words (including the manuscript text, title page, abstract, and figure legends), and 15 references. Supporting information (SI) is limited to extended methods, essential supporting datasets, and videos (no additional tables or figures).
%
%
% \subsection*{References}
%
% References should be cited in numerical order as they appear in text; this will be done automatically via bibtex, e.g. \cite{belkin2002using} and \cite{berard1994embedding,coifman2005geometric,phdthesis,masterthesis}. All references cited in the main text should be included in the main manuscript file.
%
% \subsection*{Data Archival}
%
% PNAS must be able to archive the data essential to a published article. Where such archiving is not possible, deposition of data in public databases, such as GenBank, ArrayExpress, Protein Data Bank, Unidata, and others outlined in the \href{https://www.pnas.org/author-center/editorial-and-journal-policies#materials-and-data-availability}{Information for Authors}, is acceptable.
%
% \subsection*{Language-Editing Services}
% Prior to submission, authors who believe their manuscripts would benefit from professional editing are encouraged to use a language-editing service (see list at https://www.pnas.org/author-center/language-editing). PNAS does not take responsibility for or endorse these services, and their use has no bearing on acceptance of a manuscript for publication.
%
% \begin{figure}[t!]
% \centering
% \includegraphics[width=.8\linewidth]{frog}
% \caption{Placeholder image of a frog with a long example legend to show justification setting.}
% \label{fig:frog}
% \end{figure}
%
%
% \begin{SCfigure*}[\sidecaptionrelwidth][t]
% \centering
% \includegraphics[width=11.4cm,height=11.4cm]{frog}
% \caption{This legend would be placed at the side of the figure, rather than below it.}\label{fig:side}
% \end{SCfigure*}
%
% \begin{table}[t!]
% \centering
% \caption{Comparison of the fitted potential energy surfaces and ab initio benchmark electronic energy calculations}
% \begin{tabular}{lrrr}
% Species & CBS & CV & G3 \\
% \midrule
% 1. Acetaldehyde & 0.0 & 0.0 & 0.0 \\
% 2. Vinyl alcohol & 9.1 & 9.6 & 13.5 \\
% 3. Hydroxyethylidene & 50.8 & 51.2 & 54.0\\
% \bottomrule
% \end{tabular}
%
% \addtabletext{nomenclature for the TSs refers to the numbered species in the table.}
% \end{table}
%
%
% \subsection*{Digital Figures}
%
% EPS, high-resolution PDF, and PowerPoint are preferred formats for figures that will be used in the main manuscript. Authors may submit PRC or U3D files for 3D images; these must be accompanied by 2D representations in TIFF, EPS, or high-resolution PDF format. Color images must be in RGB (red, green, blue) mode. Include the font files for any text.
%
% Images must be provided at final size, preferably 1 column width (8.7cm). Figures wider than 1 column should be sized to 11.4cm or 17.8cm wide. Numbers, letters, and symbols should be no smaller than 6 points (2mm) and no larger than 12 points (6mm) after reduction and must be consistent.
%
% Figures and tables should be labelled and referenced in the standard way using the \verb|\label{}| and \verb|\ref{}| commands.
%
% Figure \ref{fig:frog} shows an example of how to insert a column-wide figure. To insert a figure wider than one column, please use the \verb|\begin{figure*}...\end{figure*}| environment. Figures wider than one column should be sized to 11.4 cm or 17.8 cm wide. Use \verb|\begin{SCfigure*}...\end{SCfigure*}| for a wide figure with side legends.
%
% \subsection*{Tables}
% Tables should be included in the main manuscript file and should not be uploaded separately.
%
% \subsection*{Single column equations}
%
% Authors may use 1- or 2-column equations in their article, according to their preference.
%
% To allow an equation to span both columns, use the \verb|\begin{figure*}...\end{figure*}| environment mentioned above for figures.
%
% Note that the use of the \verb|widetext| environment for equations is not recommended, and should not be used.
%
% \begin{figure*}[bt!]
% \begin{align*}
% (x+y)^3&=(x+y)(x+y)^2\\
%        &=(x+y)(x^2+2xy+y^2) \numberthis \label{eqn:example} \\
%        &=x^3+3x^2y+3xy^3+x^3.
% \end{align*}
% \end{figure*}
%
%
%
% \subsection*{Supporting Information (SI)}
%
% Authors are limited to the following types of SI: extended methods, datasets, videos, and 3D figures. Extended discussion is not permitted.
%
% \subsection*{SI Appendix}
%
% Supply extended materials and methods information in a separate SI Appendix PDF file. Include SI movie legends. SI will be published as provided by the authors; it will not be edited or composed. Supplementary figures and tables are not allowed.
%
%
% \subsubsection*{SI Datasets}
%
% Supply .xlsx, .csv, .txt, .rtf, or .pdf files. This file type will be published in raw format and will not be edited or composed.
%
%
% \subsubsection*{SI Movies}
%
% Supply Audio Video Interleave (avi), Quicktime (mov), Windows Media (wmv), animated GIF (gif), or MPEG files and and include a brief legend for each movie in the main manuscript file. All movies should be submitted at the desired reproduction size and length. Movies should be no more than 10 MB in size.
%
%
%
%
% \matmethods{Please describe your materials and methods here. This can be more than one paragraph, and may contain subsections and equations as required.
%
% \subsection*{Subsection for Method}
% Example text for subsection.
% }
%
% \showmatmethods{} % Display the Materials and Methods section

\acknow{This work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.}

\showacknow{} % Display the acknowledgments section

% \bibsplit[3]
% %Use \bibsplit to split the references from the body of the text. Value "[3]" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.)


% Bibliography
% \bibliography{pnas-sample}

\bibliography{../references}
\end{document}
